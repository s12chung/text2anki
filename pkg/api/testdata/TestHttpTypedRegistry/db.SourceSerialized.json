{
  "db.SourcePart": {
    "tokenized_texts": "db.TokenizedText"
  },
  "db.SourceSerialized": {
    "created_at": "time.Time",
    "id,omitempty": "int64",
    "name": "string",
    "parts": "db.SourcePart",
    "updated_at": "time.Time"
  },
  "db.TokenizedText": {
    "Text": "text.Text",
    "tokens": "tokenizers.Token"
  },
  "text.Text": {
    "previous_break,omitempty": "bool",
    "text": "string",
    "translation": "string"
  },
  "time.Time": {},
  "tokenizers.Token": {
    "length": "uint",
    "part_of_speech": "lang.PartOfSpeech",
    "start_index": "uint",
    "text": "string"
  }
}